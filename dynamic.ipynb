{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf3H6VJOXNIA"
   },
   "outputs": [],
   "source": [
    "# 'save' or 'rerun'\n",
    "save_or_rerun = 'save'\n",
    "\n",
    "# 'passengers'\n",
    "dataset = 'passengers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf3H6VJOXNIA"
   },
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "train_size = int(12*4)\n",
    "\n",
    "hermite_points = 2\n",
    "montecarlo_runs = 10\n",
    "variational_variance = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "experiment_key = datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "experiment_folder = Path('results') / dataset / experiment_key\n",
    "print('experiment key:', experiment_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRdszBxtXNH3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "import gpflow\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from library.gplvm import GPLVM\n",
    "from library.expectations import AnalyticExpectation, GaussHermiteExpectation, UnscentedExpectation, MonteCarloExpectation\n",
    "import library.metrics\n",
    "from library.helper import plot_process\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqXHjSrZXNH9"
   },
   "outputs": [],
   "source": [
    "def lag_dataframe(d,i):\n",
    "    if i == 0:\n",
    "        return d\n",
    "    else:\n",
    "        return d.shift(i).rename(lambda c: f'{c}_lag{i}', axis='columns')\n",
    "\n",
    "def add_lag(lag,*data):\n",
    "    return pd.concat([\n",
    "        lag_dataframe(x,i) for x in data for i in range(0,lag)\n",
    "    ], axis=1)\n",
    "\n",
    "def kernel_name(k):\n",
    "    if type(k) is gpflow.kernels.Sum:\n",
    "        return '+'.join([kernel_name(k) for k in k.kernels])\n",
    "    if type(k) is gpflow.kernels.Product:\n",
    "        return '*'.join([kernel_name(k) for k in k.kernels])\n",
    "    if type(k).__name__ == 'MLP':\n",
    "        return f'MLP{k.layers}'\n",
    "    else:\n",
    "        return type(k).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMseYFPJXNIC"
   },
   "outputs": [],
   "source": [
    "dataset_path_friendly = dataset.replace(' ','_')\n",
    "if save_or_rerun not in ['save','rerun']:\n",
    "    raise Exception(f'Invalid operation {save_or_rerun}')\n",
    "if dataset == 'passengers':\n",
    "    y = pd.read_csv('data/international-airline-passengers.csv')\n",
    "    y['Month'] = pd.to_datetime(y['Month'])\n",
    "    y = y.sort_values('Month').set_index('Month')\n",
    "    y['passengers in thousands'] = y['passengers in thousands'].astype(float)\n",
    "    t = y.index\n",
    "else:\n",
    "    raise Exception(f'Unknown Dataset {dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiKi1NLlXNIF"
   },
   "outputs": [],
   "source": [
    "def windowfy(size, data):\n",
    "    windowfied_data = pd.concat({0:data}, axis=1, names=['lag', *data.columns.names])\n",
    "    for i in range(1, size):\n",
    "        windowfied_data = pd.concat([windowfied_data, pd.concat({i:data.shift(i)}, axis=1)], axis=1)\n",
    "    return windowfied_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCQ426v1XNIK"
   },
   "outputs": [],
   "source": [
    "X = windowfy(window_size,y)\n",
    "X_train = X.iloc[window_size-1:window_size+train_size-1].values\n",
    "\n",
    "X_train_var = variational_variance*np.ones(X_train.shape)\n",
    "y_train = y.iloc[window_size:window_size+train_size].values\n",
    "\n",
    "y_scaler = sk.preprocessing.StandardScaler()\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "X_train = (X_train - y_scaler.mean_)/y_scaler.scale_\n",
    "\n",
    "inducing_points = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_quzGCkXNIN"
   },
   "outputs": [],
   "source": [
    "kernels = [\n",
    "    gpflow.kernels.RBF(window_size, ARD=True) + gpflow.kernels.Linear(window_size, ARD=True),\n",
    "    gpflow.kernels.Periodic(window_size) + gpflow.kernels.RBF(window_size, ARD=True) + gpflow.kernels.Linear(window_size, ARD=True),\n",
    "]\n",
    "expectations_and_runs = [\n",
    "    (AnalyticExpectation(), 1),\n",
    "    (UnscentedExpectation(), 1),\n",
    "    (GaussHermiteExpectation(hermite_points, din=window_size), 1),\n",
    "    (MonteCarloExpectation(hermite_points**window_size), montecarlo_runs),\n",
    "    (MonteCarloExpectation(200), montecarlo_runs),\n",
    "    (MonteCarloExpectation(hermite_points*window_size), montecarlo_runs)\n",
    "]\n",
    "\n",
    "def make_model_params(k, expectation=None, predict_mode='GP'):\n",
    "    return {\n",
    "        'expectation': expectation, 'train_mode': 'GP', 'predict_mode': predict_mode,\n",
    "        'X_mean': X_train.copy(), 'X_var': X_train_var.copy(), 'Y': y_train,\n",
    "        'kern': k, 'Z': inducing_points.copy(), 'M': inducing_points.shape[0]\n",
    "    }\n",
    "\n",
    "def init_kernel_hypers(k):\n",
    "    if type(k) is gpflow.kernels.Sum:\n",
    "        for subk in k.kernels:\n",
    "            init_kernel_hypers(subk)\n",
    "    elif isinstance(k, gpflow.kernels.Stationary):\n",
    "        k.lengthscales = 10*np.sqrt(np.var(X_train, axis=0))\n",
    "        k.variance = np.var(y_train)\n",
    "    elif isinstance(k, gpflow.kernels.Static):\n",
    "        pass\n",
    "    elif type(k) is gpflow.kernels.Linear:\n",
    "        k.variance = (1/X_train.shape[1])*1e-2*np.var(y_train)*np.ones(window_size)\n",
    "    elif type(k) is gpflow.kernels.Periodic:\n",
    "        k.variance = np.var(y_train)\n",
    "    elif type(k).__name__ == 'MLP':\n",
    "        k.rbf.variance = np.var(y_train)\n",
    "    else:\n",
    "        raise Exception(f\"Can't handle {type(k)}\")\n",
    "\n",
    "gp_narxs = pd.Series({kernel_name(k): GPLVM(**make_model_params(k)) for k in kernels})\n",
    "for m in gp_narxs:\n",
    "    m.likelihood.variance = 0.01 * np.var(y_train)\n",
    "    m.X_mean.trainable = False\n",
    "    m.X_var.trainable = False\n",
    "    m.feature.trainable = False\n",
    "    init_kernel_hypers(m.kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "7XZOS7zxXNIO",
    "outputId": "fbaa1af2-9d2c-4315-82bb-e12c796e0fc2"
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "for m in gp_narxs:\n",
    "    ipd.display(ipd.HTML(f'<h4>{kernel_name(m.kern)}</h4>'))\n",
    "    print('Start',datetime.datetime.now().strftime(\"%I:%M %p\"))\n",
    "    %time gpflow.training.ScipyOptimizer().minimize(m, maxiter=5000)\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "for m in gp_narxs:\n",
    "    m.X_var.value[:,:] = m.likelihood.variance.value\n",
    "print('Actual finish', datetime.datetime.now().strftime(\"%I:%M %p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpKzyZpsXNIS"
   },
   "outputs": [],
   "source": [
    "gplvms = pd.Series({\n",
    "    ('GPLVM', e.__name__, kernel_name(k), i): GPLVM(**make_model_params(k,e,predict_mode='GPLVM'))\n",
    "    for e,r in expectations_and_runs\n",
    "    for k in kernels\n",
    "    for i in range(r)\n",
    "    if not (e.__name__ == 'analytic' and kernel_name(k) != 'SquaredExponential+Linear')\n",
    "})\n",
    "\n",
    "for gplvm in gplvms:\n",
    "    gplvm.assign(gp_narxs[kernel_name(gplvm.kern)].read_trainables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMpSUelXXNIU"
   },
   "outputs": [],
   "source": [
    "models_index = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        *[('NARX', 'analytic', kernel_name(k), 0) for k in kernels],\n",
    "        *gplvms.index\n",
    "    ],\n",
    "    names=['mode','expectation', 'kernel','run']\n",
    ")\n",
    "\n",
    "result_columns = pd.MultiIndex.from_tuples(\n",
    "    [(*x, r) for x in models_index for r in ['mean', 'variance']],\n",
    "    names=[*models_index.names, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVLq2WxeXNIV"
   },
   "outputs": [],
   "source": [
    "def free_simulate(m, propagate_uncertainty):\n",
    "    starting_loc = X.index[window_size-1]\n",
    "\n",
    "    means = pd.DataFrame(index=X.index, columns=X.columns, dtype=float).iloc[window_size-1:]\n",
    "    means.loc[starting_loc] = (X.loc[starting_loc] - y_scaler.mean_)/y_scaler.scale_\n",
    "\n",
    "    variances = pd.DataFrame(index=X.index, columns=X.columns, dtype=float).iloc[window_size-1:]\n",
    "    variances.loc[starting_loc] = m.likelihood.variance.value\n",
    "\n",
    "\n",
    "    for t_curr, t_next in zip(means.index, means.index[1:]):\n",
    "        if not propagate_uncertainty:\n",
    "            next_mean, next_variance = m.predict_y(means.loc[[t_curr]])\n",
    "        else:\n",
    "            next_mean, next_variance = m.predict_y_uncertain(means.loc[[t_curr]], variances.loc[[t_curr]])\n",
    "\n",
    "        means.loc[t_next,0] = next_mean.squeeze()\n",
    "        means.loc[t_next,pd.IndexSlice[1:]] = means.loc[t_curr,pd.IndexSlice[0:window_size-2]].values\n",
    "\n",
    "        variances.loc[t_next,0] = next_variance.squeeze()\n",
    "        variances.loc[t_next,pd.IndexSlice[1:]] = variances.loc[t_curr,pd.IndexSlice[0:window_size-2]].values\n",
    "        \n",
    "    return pd.concat({'mean':means[0], 'variance': variances[0]}, axis=1).swaplevel(axis=1).iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkq0m6OPXNIX"
   },
   "outputs": [],
   "source": [
    "narx_results = (free_simulate(m, propagate_uncertainty=False) for m in gp_narxs)\n",
    "gplvm_results = (free_simulate(m, propagate_uncertainty=True) for m in gplvms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxmjxnU2XNIa"
   },
   "outputs": [],
   "source": [
    "results = pd.concat(chain(narx_results, gplvm_results), axis=1)\n",
    "results.columns = result_columns\n",
    "results.loc[:, pd.IndexSlice[:,:,:,:,'mean']] = y_scaler.inverse_transform(results.loc[:, pd.IndexSlice[:,:,:,:,'mean']])\n",
    "results.loc[:, pd.IndexSlice[:,:,:,:,'variance']] = y_scaler.var_[0] * results.loc[:, pd.IndexSlice[:,:,:,:,'variance']]\n",
    "if save_or_rerun == 'save':\n",
    "    experiment_folder.mkdir(parents=True)\n",
    "    results.to_hdf(experiment_folder/'points.hdf', key='points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoZOb9jCXNIc"
   },
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({\n",
    "    'RMSE': [\n",
    "        np.sqrt(sk.metrics.mean_squared_error(y.iloc[window_size+train_size:], results.iloc[train_size:][(*k, 'mean')]))\n",
    "        if results.iloc[train_size+1:][(*k, 'mean')].isnull().any() != True\n",
    "        else np.nan\n",
    "        for k in models_index\n",
    "    ],\n",
    "    'NLPD': [\n",
    "        library.metrics.negative_log_predictive_density(\n",
    "            y.iloc[window_size+train_size:].values.reshape(-1,1),\n",
    "            results.iloc[train_size:][[(*k, 'mean')]].values,\n",
    "            results.iloc[train_size:][[(*k, 'variance')]].values,\n",
    "        )[0]\n",
    "        for k in models_index\n",
    "    ]\n",
    "}, index=models_index).sort_index(axis=1)\n",
    "\n",
    "if save_or_rerun == 'save':\n",
    "    metrics['NLPD'].unstack('run').T.describe().loc[['mean','std']].T.to_csv(experiment_folder/f'nlpd_mean.csv')\n",
    "    metrics['RMSE'].unstack('run').T.describe().loc[['mean','std']].T.to_csv(experiment_folder/f'rmse_mean.csv')\n",
    "    metrics.to_csv(experiment_folder/f'acc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "df4ZllJ9XNIe",
    "outputId": "6fb8b8fa-7b1a-4f41-eccb-a1987344ebab"
   },
   "outputs": [],
   "source": [
    "if save_or_rerun in ['save', 'loadsave']:\n",
    "    os.makedirs(f\"./results/figs/{dataset}/\", exist_ok=True)\n",
    "for (mode,e,k,run), kMetrics in metrics.sort_values(by='NLPD', kind='mergesort').iterrows():\n",
    "    figname = f'{k}_{run}'\n",
    "    print(figname, *kMetrics.items())\n",
    "    f, ax = plt.subplots(1,1, figsize=np.array([5,2])*1.5)\n",
    "    ax.plot(t,y.values.reshape(-1,1), label='Data', linestyle='-')\n",
    "    a = ax.get_ylim()\n",
    "    plot_process(ax,t.values[window_size:],results[mode,e,k,run], color='C1', label='Prediction')\n",
    "    # plot_process(ax,t.values[window_size:],[\n",
    "    #   results[mode,e,k,run]['mean'].values.reshape(-1,1), results[mode,e,k,run]['variance'].values.reshape(-1,1)\n",
    "    # ], color='C1', label='Prediction')\n",
    "    ax.set_ylim(a)\n",
    "    ax.legend()\n",
    "    if save_or_rerun in ['save', 'loadsave']:\n",
    "        plot_folders = experiment_folder/mode/e\n",
    "        plot_folders.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(plot_folders/f'{figname}.pdf', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close(f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dynamic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
